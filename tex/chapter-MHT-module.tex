%!TEX root = ../TTK4900-MHT.tex

\chapter{MHT Module}\label{chapter:mht-module}
To create a complete tracking \emph{system}, rather than a tracking \emph{algorithm}, it is often necessary  to complement the main algorithm with support modules. The system, or module if it is a part of a bigger system, presented here is an extension of the pre master project~\cite{Liland_2017}. The aim of this chapter is to provide a complete walkthrough of the the track oriented MHT system developed in this thesis. The motion model which is used throughout the entire tracking system when predicting and filtering target behaviour is presented first. Next follows an overview of the algorithm used to initiate new tracks into the MHT algorithm, followed by the entire MHT tracking algorithm with all its sub-routines.

\section{Motion Model}\label{sec:motion-model}
\subsection{Reference frame}
A local Cartesian NED-frame, like \gls{utm} will be used throughout this thesis, with the assumption than all input sensors are transformed to this frame (see Section~\ref{subsec:frame_conversion}). This local projection from a geodetic coordinate system to a Cartesian coordinate system is acceptable as long as the the area the system is working on is within one grid. A global geodetic frame, like WGS84 would be preferable in situations where the system tracks object over world-scale lengths but would yield non-linear equations of motion.

\subsection{Constant velocity model}
The state (\ref{eq:state_vector}) of the targets are modelled with in four dimensions in a Cartesian frame where the positive \(x\)-axis is pointing east and the positive \(y\)-axis is pointing north. The two latest states are the velocities in their respective direction.
\begin{equation}
\V{x} = \begin{bmatrix}
x & y & \dot{x} & \dot{y}
\end{bmatrix}^T
\label{eq:state_vector}
\end{equation}

Since modelling the behaviour of any ship under unknown command is next to impossible, a common assumption in tracking theory is that every target will continue on as usual, more precisely that their velocity is constant. Although simple, this model captures the essence of most vessels at sea, and when looking at maritime training~\cite{Allen2005} and regulation~\cite{IMO1972}, they both dictates that vessels should hold steady course and change course in clear decisive turns. This model is also very common in tracking applications and is used in~\cite{Reid1979,Coraluppi2000,Brekke2012,Wilthil,Vo2015,Chen2003,Habtemariam2015} among others. To give room in our model for manoeuvring, the process noise covariance is set according to the assumed manoeuvring capabilities of the vessels. This could be set as a fixed value for all targets, as done in this work, or estimated based on the history of the track or AIS information. This behaviour can be modelled as a linear time invariant system  with time evolution (\ref{eq:motion_model}), measurement model (\ref{eq:measurement_model}), transition and observation matrices (\ref{eq:model_matrices}) and systen and measurement noise matrices (\ref{eq:noise_matrices}).
\begin{equation}\label{eq:motion_model}
\V{x}_{k+1} = \M{\Phi} \V{x}_k + \V{w}_k \qquad \V{w} \sim \mathcal{N}(0;\M{Q})
\end{equation}
\begin{equation}\label{eq:measurement_model}
\V{z}_{k+1} = \M{H}\V{x}_k + \V{v}_k \qquad \V{v} \sim \mathcal{N}(0;\M{R})
\end{equation}
\begin{equation}\label{eq:model_matrices}
\M{\Phi} =	\begin{bmatrix}
1 & 0 & T & 0 \\
0 & 1 & 0 & T \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1 \\
\end{bmatrix}
\quad
\M{H} =	\begin{bmatrix}
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
\end{bmatrix}
\end{equation}
\begin{equation}\label{eq:noise_matrices}
\M{Q}	= \sigma_v^2 \begin{bmatrix}
\frac{T^3}{3} 	& 0 				& \frac{T^2}{2}	& 0 			\\
0 				& \frac{T^3}{3}  	& 0 			& \frac{T^2}{2}	\\
\frac{T^2}{2}	& 0					& T				& 0				\\
0				& \frac{T^2}{2}		& 0				& T				\\
\end{bmatrix}
\quad
\M{R} =	\sigma_m^2 
\begin{bmatrix}
1 & 0 \\
0 & 1 \\
\end{bmatrix}
\end{equation}
\begin{equation*}
\begin{split}
\M{\Phi} 	&= \text{state transition matrix} \\
\M{H}		&= \text{state observation matrix} \\
\M{Q}		&= \text{system covariance matrix} \\
\V{w}		&= \text{process noise} \\
\V{v}		&= \text{measurement noise} \\
\V{z}		&= \text{measurement vector} \\
k 			&= \text{time index} \\
T  			&= \text{time step} \\
\end{split}
\end{equation*}

\section{Track Initiation}
In comparison with \gls{homht} who treats every measurement as a potential new track since its hypotheses are essentially different ways of organizing its measurements into tracks, which are repeated for every iteration, \gls{tomht} does not have any built-in initialization of tracks since it only maintains an already existing track with track splitting and measurement-to-track association for every iteration. To remedy this lack, we need an algorithm that can find consistent and predictable patterns in an assumed uniformly distributed measurement space of clutter. 

In this work, new tracks are initiated with 2/2 \& m/n logic~\cite{Vo2015} on the unused measurements after each MHT iteration. As the name of the method indicates, this is a two step verification, where the first act as a rough filter and the second as a fine filter. As one of the main assumptions in most tracking systems, the measured radar clutter is assumed uniformly distributed in the measurement area. This assumption is quite rough in the radial measurement space, and even worse approximation in Cartesian measurement space, as illustrated in \Cref{fig:clutter_radial,fig:clutter_cartesian}.  Although far from perfect, tests shows that we still get satisfactory performance from the 2/2\&m/n method.
\begin{figure}[H]
\centering
\begin{minipage}{0.45\textwidth}
\includegraphics[width=\textwidth]{Figures/clutterRadial.png}
\caption{Uniform radial clutter}\label{fig:clutter_radial}
\end{minipage}\hfill
\begin{minipage}{0.45\textwidth}
\includegraphics[width=\textwidth]{Figures/clutterCartesian.png}
\caption{Uniform Cartesian clutter}\label{fig:clutter_cartesian}
\end{minipage}
\end{figure}
The flow of the method is illustrated in Figure~\ref{fig:init_flowchart}, and for better clarity, the algorithm is explained from the last step to the first step, since this is the sequence a newly started initiation algorithm will perform its operations.
\begin{figure}[H]
\centering
\includegraphics[width = .9\textwidth]{init_flowchart.pdf}
\caption{2/2\&m/n flowchart}\label{fig:init_flowchart}
\end{figure}

\subsection{Spawn new initiators}
All measurement unused by the `Process preliminary tracks' and `Process initiators' steps will be the basis for new \emph{initiators}. An initiator is a measurement that that awaits its match in the next scan. The idea is that uniformly distributed clutter will not (often) reappear at approximately the same location two times in a row, effectively filtering out most of the clutter.

\subsection{Process initiators}
When the next scan arrives, all the unused measurements from the `Process preliminary tracks' step will be used as candidates in this step. Since an initiator is only a position and not a full state with velocity, all directions are equally likely, and the only design parameter in this step is maximum speed of targets to be tracked. This parameter sets an outer limit on the circle acting as a gate for the second and confirming measurement. When matching initiators with a second measurement, we want to select the closest measurement, making the assumption that the two consecutive measurements are the most likely to belong together. In a single target scenario, where this would be to calculate the distance to all the alternative measurements and select the lowest, the association is already done. While in a multitarget scenario, we \emph{could} select the closest measurement to any initiator, but we would have to do this one initiator at a time. This would lead to different results depending on the arrangement of the initiators in the programming of this method. A different approach would be to calculate all the different distances for any possible combination of initiators and measurements, sort the list, and assign the distances from the shortest to the longest possible distances. This approach would not be influenced by randomness like the arrangement of the initiators in a programming language, but would not necessarily give the global optimal association regarding the how many initiators that are assigned measurements and their respective distances.

Since we can have situations like exemplified in Figure~\ref{fig:init_gating}, where two initiators have the same measurement inside their gates, and one of them have a second measurement inside its gate, we need to take the global consequence of any assignment into consideration.
\begin{figure}
\centering
\includegraphics[width = .8\textwidth]{Figures/init_gating.pdf}
\caption{Initiator gating example}\label{fig:init_gating}
\end{figure}
If using method 1; to sequentially select the best, we have two possible outcomes. When starting with initiator 1, this initiator would be associated with measurement 2, and initiator 2 would not be associated with any measurements. On the other hand, starting with initiator 2 would lead to this initiator being associated with measurement 2, and initiator 1 would be associated with measurement 1. This randomness in outcome based on which initiator the algorithm starts with is clearly not a desired property. If using using method 2; to sequentially select the globally shortest distance, we would first associate initiator 1 with measurement 2, and there would not be any measurements left for initiator 2, leaving this empty.

A third option is to formulate the problem as a global combinatorial problem, and use an `off-the-shelf' solution to solve the problem. We have essentially a matrix with initiators along one axis and measurements along the second axis and the distance between them in their intersections, as in (\ref{eq:init_assignment_matrix}) for our example.
\begin{equation}\label{eq:init_assignment_matrix}
\kbordermatrix{
	 	& M_1 	& M_2 	& M_3	\\
    I_1 & 3 	& 1 	& 5 	\\
    I_2 & 7 	& 2 	& 6		\\
}
\end{equation}
The values above the threshold set by the maximum speed multiplied with the time period between the radar scans can be set to infinity to symbolise that this combination is not possible, see (\ref{eq:gated_init_assignment_matrix}) where the gate threshold is 4.
\begin{equation}\label{eq:gated_init_assignment_matrix}
\kbordermatrix{
	 	& M_1 		& M_2 	& M_3		\\
    I_1 & 3 		& 1 	& \infty 	\\
    I_2 & \infty 	& 2 	& \infty	\\
}
\end{equation}
If we remove the columns with only infinity, we are removing measurements that cannot be associated under any circumstances, thus reducing the size of the problem, see (\ref{eq:masked_gated_init_assignment_matrix}). With this pre processing, we want to assign each row to a column so that the sum of the selected intersections are minimal. 
\begin{equation}\label{eq:masked_gated_init_assignment_matrix}
\kbordermatrix{
	 	& M_1 		& M_2	\\
    I_1 & 3 		& 1  	\\ 
    I_2 & \infty 	& 2 	\\
}
\end{equation}
We now have formulated our problem in a way that it can be solved by the `Hungarian'\footnote{also known as the Munkres or Kuhn-Munkres algorithm} algorithm~\cite{Munkres1957}, which will give us the association \(I_1 \rightarrow M_1\) and \(I_2 \rightarrow M_2\). From the associations, a full state is calculated and a new preliminary track is created. A preliminary track contains a state, covariance and counters of number of checks and passed checks.

\subsection{Process preliminary tracks}
When a new set of unused measurements arrive from the tracker, all the preliminary tracks are predicted to the time of the measurements. We now have the same association challenge between the predicted states and the measurements as with the initiators and measurements. Since we now have a full state and covariance for every preliminary track, we calculate the \gls{nis} for every combination of preliminary tracks and measurements, and selects the best combination. The preliminary tracks that is associated with a new measurement, their passed counter is incremented with one, while all preliminary tracks' checks counter is incremented with one. 

For preliminary tracks that have enough passed measurements a new initial target is sent to the tracker. All preliminary tracks with check counter above the threshold is categorized as dead and deleted. 

\section{MHT Overview}
The aim of this section is to outline the major steps in the MHT module and the flow of data and decisions. Figure~\ref{fig:algorithm_flow} shows the main steps that the module perform at each iteration / radar scan. When new measurements are received, all track hypotheses / leaf nodes are predicted forward to the time of the radar measurements. The measurements are then gated for each hypothesis, and new hypotheses are generated for measurements within the gate. Each new hypothesis is then given a score, which is an accumulation of the parent node score and the new node's score. The target trees are then clustered according to which trees that shares measurements, whereon clusters with only one tree has the option of removing / merging similar hypotheses to reduce the size of the tree. For each cluster, the cluster-wise globally best association combination is selected using \gls{ilp}. Then, for each selected hypothesis the parent N steps above becomes the new root of that tree, and the children to the previous root node are removed. The sliding window size (N) could be a static design parameter or a function of the runtime of that tree, which reflects the overall size of the tree. This enables the system to adapt its core parameters to guarantee its runtime demands. Next, targets who's  best hypothesis have a score below the threshold is terminated, followed by the initialization of new targets from the initiator module. 
\begin{figure}[H]
\centering
\includegraphics[width = \textwidth]{algorithm_flowchart.pdf}
\caption{Algorithm flowchart}\label{fig:algorithm_flow}
\end{figure}

\section{Predict target position}
To compare new measurements with existing hypotheses, we should predict their states to the same time as the measurements, which can be done with the Kalman filter `time update' equation (\ref{eq:kalman_timeUpdate}). The residual covariance and optimal kalman gain (\ref{eq:kalman_measurementUpdate_precalc}), which is a part of the `measurement update' sequence of a Kalman filter are also calculated as the residual covariance is needed in the gating.
\begin{equation}\label{eq:kalman_timeUpdate}
\begin{split}
\V{\bar{x}}_{k+1} 	&= \M{\Phi} \V{\hat{x}}_k \\
\M{\bar{P}}_{k+1}	&= \M{\Phi} \M{\hat{P}}_k  \M{\Phi}^T + \M{Q} \\
\end{split}
\end{equation}
\begin{equation}\label{eq:kalman_measurementUpdate_precalc}
\begin{split}
\M{S}_k	&= \M{H}\M{\bar{P}}_k \M{H}^T + \M{R} \\
\M{K}_k &= \M{\bar{P}}_k \M{H}^T \M{S}_k^{-1} \\
\end{split}
\end{equation}

\begin{equation*}
\begin{split}
\V{\bar{x}}	&= \text{predicted state} \\
\V{\hat{x}} &= \text{filtered state} \\
\M{\bar{P}} &= \text{predicted state covariance} \\
\M{\hat{P}} &= \text{filtered state covariance} \\
\M{Q}		&= \text{system noise covariance} \\
k  			&= \text{time index} \\
\end{split}
\end{equation*}

\section{Gate and Create new hypotheses}
To limit the number of hypotheses each leaf node have make, the measurements are gated based on the leaf nodes predicted covariance and a set confidence level. The size of the gate (Figure~\ref{fig:gate_illustration}) will reflect how insecure the prediction is, which is a function of how many detections and missed detections the leaf node have had. The gate is defined as \gls{nis} less that a threshold set by the \(\chi^2\)--distribution \gls{cdf} with two degrees of freedom and a set confidence value. A set of confidence levels and belonging \(\chi^2\) \gls{cdf} values are listed in Table~\ref{tab:chi_square}.
\begin{equation}\label{eq:gate}
\begin{gathered}
\V{\tilde{z}} = \V{z} - \M{H}\V{\bar{x}} \\
NIS = \V{\tilde{z}}^T	\M{S}^{-1} \V{\tilde{z}} \leq \eta^2
\end{gathered}
\end{equation}
\begin{equation*}
\begin{split}
\V{\tilde{z}}	&= \text{Measurement residual}  \\
\eta^2 			&= \text{Inverse \(\chi^2\) \gls{cdf}} \\
\end{split}
\end{equation*}
\begin{table}
\centering
\begin{tabular}{c c c c c c c c}
Confidence 	& 70\% 	& 80\% 	& 90\% 	& 95\% 	& 97.5\% 	& 99\% 	& 99.5\% \\ 
\midrule
\(\eta^2\) 	& 2.41 	& 3.22 	& 4.61 	& 5.99 	& 7.38 		& 9.21 	& 10.60
\end{tabular}\caption{Inverse \(\chi^2\) \gls{cdf} for two degrees of freedom}
~\label{tab:chi_square}
\end{table}
\begin{figure}
\centering
\includegraphics[width = .7\textwidth]{Figures/gate_without_measurement.png}
\caption{Predicted state and gate}\label{fig:gate_illustration}
\end{figure}

\subsection{Zero hypothesis}
To account for the possibility that the target is not present in this scan, a \emph{zero} hypothesis, or \emph{dummy} hypothesis as it is sometimes called, is generated with the predicted state and covariance.

\subsection{Pure radar hypotheses}
For every radar measurement inside the gate in (\ref{eq:gate}), a new track hypothesis is generated with filtered state and covariance according to regular Kalman measurement update equations (\ref{eq:kalman_measurementUpdate}).
\begin{equation}
\begin{split}
\V{\tilde{y}}	&= \V{z} - \M{H} \V{\bar{x}} \\
\M{S}			&= \M{H} \M{\bar{P}} \M{H}^T + \M{R} \\
\M{K} 			&= \M{\bar{P}} \M{H}^T \M{S}^{-1} \\
\V{\hat{x}}(k) 	&= \V{\bar{x}} + \M{K} \V{\tilde{y}} \\
\M{\hat{P}}(k) 	&= \left( \M{I} - \M{K} \M{H} \right) \M{\bar{P}}
\end{split}
\label{eq:kalman_measurementUpdate}
\end{equation}

\subsection{Combined radar and AIS hypotheses}\label{subsec:combined_radar_and_ais_hypotheses}
As elaborated in Section~\ref{sec:ais_preprocessing} all AIS measurements are preprocessed to remove out-of-order messages and ID-swap errors. And for each radar scan, only the latest AIS update from each target (\gls{mmsi} number) are passed through to the MHT tracking loop.

For every (predicted) \gls{ais} measurement, there are generated a new complete set of hypotheses consisting of the \gls{ais} measurement and all the radar measurements inside the gate. This will lead to a large number of new hypotheses, but since the number of \gls{ais} measurements inside any gate at any time will seldom be larger than one, and in most cases zero, this will not cause any substantially larger explosion in the number of track hypotheses that the already exponentially nature of any \gls{mht}.% Since both the radar- and \gls{ais} measurement have the same process noise, the fused estimate will have an uncertainty area about 70 percent of the pure radar measurement, whereas the uncertainty would be 50 percent without common process noise~\cite{Bar-Shalom1986}.

Since the AIS measurements originates before the radar measurement, the fusion is carried out in two steps as a sequential update~\cite{Bar-Shalom1995}. 
\begin{figure}[H]
\centering
\includegraphics[width = .8\textwidth]{Figures/AIS_gating.pdf}
\caption{AIS gating}\label{fig:ais_gating}
\end{figure}
To get a more accurate score when summing two measurements, we are first predicting the origin hypothesis to the time of the AIS measurement. This predicted state is then filtered with the AIS measurement, giving rise for a new state, covariance and \gls{nis}. The \gls{ais} score is then calculated based on how good this intermittent prediction matches the AIS measurement. The new state is then predicted to the time of the radar measurement, which is most likely not at the same location as the original prediction. This prediction is then filtered with the radar measurement, and a radar score is calculated. The new hypothesis are given the accumulative score for the radar and \gls{ais}. This process is repeated for each new radar measurement in the gate.

\subsection{Pure AIS hypotheses}\label{subsec:pure_ais_hypotheses}
If no radar measurements are present in the gate while there are AIS measurements present inside the gate, pure AIS hypotheses are created. This can be the situation when a target is broadcasting an AIS message, but is either in radar shadow or is not detected by the radar for any reason. These hypotheses are not created when one or more radar measurements are available, based on the assumption that if a radar measurement is present, the difference between a fused hypothesis and a pure AIS hypothesis is quite small since the AIS measurement covariance typically will be much smaller than the radar measurement covariance, leading to a fused state very close to the AIS measurement.

\begin{figure}[H]
\centering
\includegraphics[height = .3\textheight]{Figures/Hypotheses_when_turning.PNG}
\caption{Hypotheses when turning}\label{fig:hypotheses_when_turning}
\end{figure}

\section{Scoring}
The scoring used in this tracking system is based on a dimensionless score function by Bar-Shalom~\cite{Bar-Shalom2007}. His paper discusses the issue of scoring measurement-to-track associations and comparing scores based on different numbers of measurement and measurement dimensions. He proposes a dimensionless \emph{likelihood ratio}, which is the \gls{pdf} of a measurement having originating from the track, to the \gls{pdf} of it not originating from the track.~\todo[inline]{Fill in the main steps in the derivation}

Each \gls{track hypothesis} is scored according to (\ref{eq:score_function}). Where the cumulative score being the sum through time since we are using the logarithm of the likelihood ration. For the fused hypotheses, their score is the cumulative score of both the measurements, giving them a better score reflecting that they are more likely hypotheses that pure radar or AIS hypotheses. 
\begin{equation}\label{eq:score_function}
\begin{split}
\mathrm{NLLR} &= \frac{1}{2} \left[ {\tilde{z}}^{T} {S}^{-1} \tilde{z} \right] + \ln \frac{\lambda_{ex} |2 \pi S|^{1/2}} {P_D} \\				
\tilde{z} &= z -\hat{z}
\end{split}
\end{equation}
\begin{equation}
\mathrm{cNLLR}_k^j \triangleq \sum_{l=0}^k NLLR_{i,j}(l)
\end{equation}

\section{Clustering}
The problem of finding the globally optimal set of track hypotheses increases exponentially with the number of hypotheses in the problem. To reduce the size of the problem, it is desirable to split it into smaller independent problems. Both because it enables parallel computation and it reduces the total cost of solving the problem. Track trees that have common measurements must be solved together, since they can have mutual exclusive leaf nodes. (Remember that each target can maximum produce on radar measurement at each scan.) The clustering can be done efficiently through \gls{bfs} or \gls{dfs} on a graph made from the hypothesis tree.

By constructing a 0--1 adjacency matrix describing the connection between all the nodes in the track forest, the clustering problem is equivalent to the \emph{connected components} problem in graph theory~\cite{Chen2015}.

\section{Optimal data association}
When the targets are divided into independent clusters, each of them can be treated as a global problem where we want to minimize the cost or maximize the score of the selected \glspl{track hypothesis} (leaf nodes). The selected \glspl{track hypothesis} must also fulfil the constraints, that each measurement can only be a part of one track, and that minimum and maximum one track hypothesis can be selected from each target. Since only binary values, selected or not selected, is possible for selection of hypotheses, the problem becomes an \gls{ilp}. In the case where a cluster is only containing one target tree, the best hypothesis can be selected by running a search among the leaf nodes after the highest score, since none of the leaf nodes are excluding other leaf nodes in other target trees. This will often be the case for targets that are largely spaced out, and their gates are not and have not overlapped in a while. For any other case, where there are two or more targets in a cluster, the procedure in Section~\ref{subsec:integer_linear_programming} must be carried out. 

\subsection{Integer Linear programming}\label{subsec:integer_linear_programming}
The essence of any optimization problem is a cost function and a set of constraints. In our problem, we want to select the combination of hypotheses (leaf nodes) that gives the highest score / lowest cost, while not selecting any measurement more than one time and ensure that we select minimum and maximum one hypothesis from each target.


%%%%% CONTINUE HERE %%%%%%%

Our cost vector \(\V{c}\) is 

 them all is (\ref{eq:general_objective_funtion}), where \(\V{c}\) is a vector of costs (minimize) or scores (maximize) and \(\V{\tau}\) is a selection vector, where each row in \(\V{c}\) and \(\V{\tau}\) represents one branch in the track hypothesis tree.
\begin{equation}
\begin{aligned}
& \underset{\V{\tau}}{\text{min}}
& & \V{c}^T \V{\tau}
\end{aligned}
\label{eq:general_objective_funtion}
\end{equation}

There are two sets of constraints (\ref{eq:my_constraints}), one equality and one inequality. The inequality constraints \(\M{A_1} \V{\tau} \leq \V{b_1}\) ensures that each measurement are maximum (but not minimum) used one time. The equality constraints \(\M{A_2} \V{\tau} = \V{b_2}\) ensures that minimum and maximum one track from each track tree is selected. The complete \gls{ilp} formulation becomes (\ref{eq:my_constraints}), where \(\V{\tau}\) is a binary vector with dimension equal the number of leaf nodes in the track forest.
\begin{equation}
\begin{aligned}
&	\underset{\V{\tau}}{\text{max}}
&&	\V{c}^T \V{\tau} \\
&	\text{s.t.}
&&	\M{A_1} \V{\tau} \leq \V{b_1} 	\\
&&&	\M{A_2} \V{\tau} = \V{b_2}	\\
&&&	\V{\tau} \in {\{0,1\}}^{M}
\end{aligned}
\label{eq:my_constraints}
\end{equation}
\(\M{A_1}\) is a \(n_1 \times m\) binary matrix with \(n_1\) real measurements and \(m\) track hypotheses (all leaf nodes), where \(\M{A_1}(l,i)=1\) if hypothesis \(l\) are utilizing measurement \(i\), \(0\) otherwise. The measurements and hypothesis are indexed by the order they are visited by \gls{dfs}. \(\M{A_2}\) is an \(N_2 \times m\) binary matrix where \(N_2\) is the number of targets in the cluster and \(\M{A_2}(l,j)=1\) if hypothesis \(l\) belongs to target \(j\). \(\V{b_1}\) is a \(n_1\) long vector with ones and \(\V{b_2}\) is a \(N_2\) long vector with ones. \(\V{c}\) is a \(m\) long vector with a measure of the goodness of the track hypotheses. For example, in Figure~\ref{fig:hyp-tree} at time step 2, the \(\M{A}\) matrices and \(\V{C}\) vector would be (\ref{eq:example_matrices}).
\begin{figure}[H]
\centering
\includegraphics[clip, trim=0cm 2.5cm 0cm 2.5cm, width = .7\textwidth]{Track-tree}
\caption{Track hypothesis tree}\label{fig:hyp-tree}
\end{figure}

\begin{equation}
\begin{split}
\M{A_1} &=\begin{bmatrix}
		0 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\
       	0 & 0 & 1 & 0 & 0 & 1 & 0 & 1 & 0 \\
       	0 & 0 & 0 & 1 & 0 & 0 & 1 & 1 & 1 \\
       	0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 \\
     	\end{bmatrix}
\V{b_1} = 	\begin{bmatrix}
			1 \\ 1  \\ 1 \\ 1
			\end{bmatrix} \\
\M{A_2} &=\begin{bmatrix}
		1 & 1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 \\
       	0 & 0 & 0 & 0 & 1 & 1 & 1 & 1 & 1 \\
     	\end{bmatrix} 
\V{b_2} = 	\begin{bmatrix}
			1 \\ 1
			\end{bmatrix} \\
\V{c} &=\begin{bmatrix}
		\lambda_1 & \lambda_2 & \lambda_3 & \lambda_4 & \lambda_5 & \lambda_6 & \lambda_7 & \lambda_8 & \lambda_9
		\end{bmatrix}^T \\
\end{split}
\label{eq:example_matrices}
\end{equation}

The explicit enumeration that becomes necessary when creating these \(\M{A}\) matrices is exhaustive since the dimension of \(\V{\tau}$, which is equal to the number of leaf nodes in the track forest, can be very large. Both \(\M{A}_1\) and \(\M{A}_2\) grows quadratically with the number of track hypotheses and real measurements or number of targets respectively.

\subsection{Solvers}
There are a lot of off-the-shelf \gls{ilp} and \gls{milp} solvers on the marked, both free open source and commercial. Since the problem in this report is formulated on standard form, it can easily be executed on several solvers, and we can compare runtime and performance. The performance difference of some of these where tested in~\cite{Liland_2017}, where the difference where found marginal, probably because of the nature of the problem. %The read/write overhead, building problem vs. solving problem, etc. 


% \section{ILP Pruning}

\section{Dynamic window}

\section{N-Scan pruning}

\section{Track termination}

\section{Track smoothing}
\begin{figure}[H]
\centering
\includegraphics[width = .9\textwidth]{Figures/track_smoothing.png}
\caption{Track smoothing}\label{fig:track_smoothing}
\end{figure}